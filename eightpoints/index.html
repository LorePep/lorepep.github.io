<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns#">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>The eight-points algorithm &middot; </title>
        <meta name="description" content="How to compute the transformation between two camera poses">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="generator" content="Hugo 0.63.2" />
        <meta name="robots" content="index,follow">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta property="og:title" content="The eight-points algorithm">
<meta property="og:description" content="How to compute the transformation between two camera poses">
<meta property="og:type" content="article">
<meta property="og:url" content="/eightpoints/">
        <link rel="stylesheet" href="/dist/styles.css">
        <link rel="stylesheet" href="/dist/override.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,400,600,700,300&subset=latin,cyrillic-ext,latin-ext,cyrillic">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
        
        
        
        
    </head>
    <body>
        

        <div id="wrapper">
            <header class="site-header">
                <div class="container">
                    <div class="site-title-wrapper">
                        
                            <h1 class="site-title">
                                <a title="Lorenzo Peppoloni" href="/">Lorenzo Peppoloni</a>
                            </h1>
                        
                        <a class="button-square" href="/index.xml"><i class="fa fa-rss"></i></a>
                        
                            <a class="button-square button-social hint--top" data-hint="Twitter" title="Twitter" href="https://twitter.com/lorepep" rel="me">
                                <i class="fa fa-twitter"></i>
                            </a>
                        
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="Github" title="Github" href="https://github.com/lorepep" rel="me">
                                <i class="fa fa-github-alt"></i>
                            </a>
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="LinkedIn" title="LinkedIn" href="https://twitter.com/lpeppoloni" rel="me">
                                <i class="fa fa-linkedin"></i>
                            </a>
                        
                        
                        
                    </div>

                    <ul class="site-nav">
                        
    <li class="site-nav-item">
        <a title="About" href="/page/about/">About</a>
    </li>

    <li class="site-nav-item">
        <a title="CV" href="">CV</a>
    </li>

                    </ul>
                </div>
            </header>

            <div id="container">


<div class="container">
    <article class="post-container" itemscope="" itemtype="http://schema.org/BlogPosting">
        <header class="post-header">
    <h1 class="post-title" itemprop="name headline">The eight-points algorithm</h1>
    
        <p class="post-description" itemprop="description">How to compute the transformation between two camera poses</p>
    
    <p class="post-date post-line">
        <span>Published <time datetime="2020-02-19" itemprop="datePublished">Wed, Feb 19, 2020</time></span>
        <span>by</span>
        <span itemscope="" itemprop="author" itemtype="https://schema.org/Person">
            <span itemprop="name">
                <a href="#" itemprop="url" rel="author">Lorenzo Peppoloni</a>
            </span>
        </span>
    </p>
    
</header>

        <div class="post-content clearfix" itemprop="articleBody">
    

    <p>In <a href="https://lorenzopeppoloni.com/lkttracker/">this</a> blog post we had a look at how to estimate the optical flow (e.g., track how pixels move in time) in a set of images. The estimation we obtained gave us pixel matches across the image set.</p>

<p>Given the correspondences between two images, we can estimate the motion and the 3D position of the points we are observing. Solving this problem is known as Structure from Motion (SfM).</p>

<p><figure><img src="/eightpoints/pix.png" alt="Example"></figure></p>

<p>Let's assume we are observing a 3D point <span  class="math">\(X\)</span>, in two different images. The two viewpoints are related by an affine transformation (rotation plus translation) given by the matrix <span  class="math">\(R\)</span> (for rotation) and by the vector <span  class="math">\(T\)</span> for the translation. If we draw an imaginary line between the image centres <span  class="math">\(c_1\)</span> and <span  class="math">\(c_2\)</span> to the 3D point, we have that the point is projected to the point <span  class="math">\(x_1\)</span> (in image space) in the first image and to the point <span  class="math">\(x_2\)</span> (in image space) in the second image.</p>

<p>In the camera frame, we can also write that <span  class="math">\(\lambda_1 x_1 = X\)</span> and that <span  class="math">\(\lambda_2 x_2 = X\)</span>, being <span  class="math">\(\lambda_1\)</span> and <span  class="math">\(\lambda_2\)</span> the scaling factors to go from the points in the image to the point <span  class="math">\(X\)</span>.</p>

<p>So, let's say that we know <span  class="math">\(x_1\)</span> and <span  class="math">\(x_2\)</span> (one of the matches we found between the two images), how can we recover <span  class="math">\(R\)</span>, <span  class="math">\(T\)</span> and <span  class="math">\(X\)</span>?</p>

<p>Let's try and rewrite everything in the second camera frame.</p>

<p><span  class="math">\[ \lambda_2x_2 = R\lambda_1 x_1 + T\]</span></p>

<p>We can multiply for the <a href="https://en.wikipedia.org/wiki/Skew-symmetric_matrix">skew-symmetric</a> matrix of T</p>

<p><span  class="math">\[ \lambda_2 \hat{T}x_2 = \lambda_1 \hat{T}Rx_1 \]</span></p>

<p>we can then multiply for <span  class="math">\( x_2^T \)</span> and divide by <span  class="math">\(\lambda_1\)</span></p>

<p><span  class="math">\[ x_2^{T}\hat{T}Rx_1 = 0 \]</span></p>

<p>Note: the term on the left gets to zero because <span  class="math">\( T \times x_2\)</span> is orthogonal to <span  class="math">\(x_2\)</span> so if you compute the scalar product for <span  class="math">\( x_2 \)</span> you get zero.</p>

<p>Now we have an expression that couples the camera motion and the two known 2D locations. This equation is called the <strong>epipolar constraint</strong>. Note that the 3D points <span  class="math">\(X\)</span> do not appear in the equation, we successfully decoupled the problem of computing <span  class="math">\(R\)</span> and <span  class="math">\(T\)</span> from the problem of computing the 3D coordinates of <span  class="math">\(X\)</span>.</p>

<p>Geometrically, the epipolar constraint says something pretty straightforward. If you look at the first picture: the volume spanned by the vectors <span  class="math">\(x_2\)</span>, T (<span  class="math">\(\vec{o_2o_1}\)</span>) and <span  class="math">\(Rx_1\)</span> (which is <span  class="math">\(\vec{c_1x_1}\)</span> seen from the second camera) has a zero volume, thus the triangle <span  class="math">\((c_1c_2X)\)</span> lies on a plane.</p>

<p>The epipolar constraint can be rewritten as:</p>

<p><span  class="math">\[ x_2^{T}Ex_1 = 0 \]</span></p>

<p><span  class="math">\(E\)</span> is called the essential matrix, and it has the following property:</p>

<p><span  class="math">\[eig(S) = (\sigma, \sigma, 0)\]</span></p>

<p>That is the essential matrix has three eigenvalues, two are equals and one is zero.</p>

<p><span  class="math">\(R\)</span> and <span  class="math">\(T\)</span> can be extracted from the essential matrix. Usually what we do in practice is that we find a matrix <span  class="math">\(F\)</span> that solves the epipolar constraint and then we compute the &quot;closest&quot; essential matrix (projecting <span  class="math">\(F\)</span> to the space of the essential matrices).</p>

<h3 id="the-eightpoints-algorithm">The eight-points algorithm</h3>

<p>To solve the equation in <span  class="math">\(E\)</span> we need to re-write it in such a way to separate known variables (<span  class="math">\(x_1\)</span> and <span  class="math">\(x_2\)</span>) from the unknown <span  class="math">\(E\)</span>.</p>

<p>If we stack the columns of <span  class="math">\(E\)</span> in a single vector <span  class="math">\(E^{s}\)</span> and we use the <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker product</a> of <span  class="math">\(x_1\)</span> and <span  class="math">\(x_2\)</span> (<span  class="math">\(a\)</span>) we can write</p>

<p><span  class="math">\[ x_2^TEx_1 = a^{T}E^{S} = 0 \]</span></p>

<p>Now, we can stack this equation for all the matches we have between the two images and obtain the following linear system which contains all the epipolar constraints for all the points</p>

<p><span  class="math">\[ \chi E^{S} = 0 \quad \text{with } \chi = (a^{1}, a^{2}, ..., a^{n})^{T} \]</span></p>

<p>You can immediately see that the solution to the system is not unique and that every scaling factor multiplying <span  class="math">\(E^{s}\)</span> will solve the equation. In practice, this means that we are not able to compute the baseline, that is the translation between the two cameras, but only its direction. The solution is to consider the baseline equals to one and compute everything in &quot;baseline units&quot;.</p>

<p>To have a unique solution at this points we need at least 8 points (that's what gives the name to the algorithm)</p>

<p>Once we solved for a generic matrix <span  class="math">\(F\)</span>, we can find the closest <span  class="math">\(E\)</span> by doing</p>

<p><span  class="math">\[
\begin{matrix}
F = U \text{diag}(\lambda_1, \lambda_2, \lambda_3) V^{T} \phantom{..........}\\
E = U \text{diag}(\sigma, \sigma, 0)V^{T} \quad \sigma = \frac{\lambda_1+\lambda_2}{2}
\end{matrix}
\]</span></p>

<p>As we said before, there is a scaling factor that we cannot reconstruct, to fix the scale we can impose <span  class="math">\(\sigma = 1\)</span>, obtaining a final essential matrix <span  class="math">\(E = U \text{diag}(1, 1, 0) V^T\)</span>.</p>

<h3 id="caveats">Caveats</h3>

<ul>
<li><span  class="math">\(E = 0\)</span> is a solution in which we collapse everything to a point, it's a valid solution but we don't care about it</li>
<li>There degenerate cases, (e.g., all the matches lie on a line or plane) where no matter how many points you have you cannot have a unique solution</li>
<li>We cannot get the sign of E (also <span  class="math">\(-kE^{s}\)</span> is a solution), so we have 4 possible combinations for R and T. The solution to the problem is to pick the <span  class="math">\(R\)</span> and <span  class="math">\(T\)</span> couple which gives positive depth values (the 3D points are in front of the camera).</li>
<li>If <span  class="math">\(T = 0\)</span>, that is there is no translation the algorithm fails, but this never happens in real life.</li>
</ul>

<p>How do we extract the possible combinations of <span  class="math">\(R\)</span> and <span  class="math">\(T\)</span>?</p>

<p>Given</p>

<p><span  class="math">\[
W = \begin{pmatrix}
0 & -1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 1 \\
\end{pmatrix}
\]</span></p>

<p>which describes a rotation of <span  class="math">\(\pi/2\)</span> aroubd <span  class="math">\(z\)</span>, we have four possible solutions given by two rotation matrices <span  class="math">\(R_1\)</span> and <span  class="math">\(R_2\)</span> and two translations <span  class="math">\(T_1\)</span> and <span  class="math">\(T_2\)</span>.</p>

<p><span  class="math">\[
R_1 = UWV^{T} \qquad R_2 = UW^{T}V^{T}
\]</span></p>

<p><span  class="math">\[
T_1 = U_3 \qquad T_2 = -U_3
\]</span></p>

<p>Let's have a look at a toy example using Python, full code <a href="https://github.com/LorePep/blogposts_code/tree/master/eight-points">here</a>.</p>

<p>Let's generate a fixture world with two cameras and eight 3D points. In the image, each frame is represented with r (x-axis), g (y-axis) and b (z-axis).</p>

<p><figure><img src="/eightpoints/3d_world.png" alt="Example"></figure></p>

<p>Now, we assume that our cameras have a focal length of one and we transform the points into the normalized image space. The resulting images for both the cameras are represented in the figure, where colours match point correspondences.</p>

<p><figure><img src="/eightpoints/images.png" alt="Example"></figure></p>

<p>From the points, we can compute the Kronecker product and extract our estimated essential matrix.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_extract_rot_transl</span>(U, V):
    W <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(([<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>], [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>], [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]))
    <span style="color:#66d9ef">return</span> [
        [np<span style="color:#f92672">.</span>dot(U, np<span style="color:#f92672">.</span>dot(W, V)), U[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]],
        [np<span style="color:#f92672">.</span>dot(U, np<span style="color:#f92672">.</span>dot(W, V)), <span style="color:#f92672">-</span>U[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]],
        [np<span style="color:#f92672">.</span>dot(U, np<span style="color:#f92672">.</span>dot(W<span style="color:#f92672">.</span>T, V)), U[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]],
        [np<span style="color:#f92672">.</span>dot(U, np<span style="color:#f92672">.</span>dot(W<span style="color:#f92672">.</span>T, V)), <span style="color:#f92672">-</span>U[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, :]],
    ]


chi <span style="color:#f92672">=</span> _compute_kronecker(points_1, points_2)
_, _, V1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svd(chi)
F <span style="color:#f92672">=</span> V1[<span style="color:#ae81ff">8</span>, :]<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)<span style="color:#f92672">.</span>T
U, _, V <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linalg<span style="color:#f92672">.</span>svd(F)
possible_r_t <span style="color:#f92672">=</span> _extract_rot_transl(U, V)</code></pre></div>
<p>Let's compare the results we got with the original rotation and translation from camera_1 to camera_2.</p>

<p>One of the solutions we get:</p>
<pre><code>R = [[ 0.95533649, -0.        ,  0.29552021],
     [ 0.0587108 ,  0.98006658, -0.18979606],
     [-0.28962948,  0.19866933,  0.93629336]]
t = [-1.,  0.,  0.]</code></pre>
<p>With the original <span  class="math">\(R\)</span> and <span  class="math">\(T\)</span>:</p>
<pre><code>R = [[ 0.95533649, -0.        ,  0.29552021],
     [ 0.0587108,   0.98006658, -0.18979606],
     [-0.28962948,  0.19866933,  0.93629336]]
t = [-1.5,  0.,  0.]</code></pre>
<p>As you can see we were able to fully recover <span  class="math">\(R\)</span> and <span  class="math">\(T\)</span>, but only up to a scaling factor.</p>

<hr>

<p><em>Conclusions: We had an in-depth look at the eight-points algorithm to reconstruct the affine transformation between two camera poses observing the same 3D points. We formally introduce the algorithm, discussed caveats and we had a look at a real example using synthetic data in Python.</em></p>

</div>

        <footer class="post-footer clearfix">
    
        <p class="post-tags">
            <span>Tagged:</span>
            
            
                <a href="/tags/computer-vision/">computer-vision</a>, 
            
                <a href="/tags/opencv/">openCV</a>, 
            
                <a href="/tags/robotics/">robotics</a>
            
        </p>
    

    <div class="share">
        
            <a class="icon-twitter" href="https://twitter.com/share?text=The%20eight-points%20algorithm&url=%2feightpoints%2f"
                onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                <i class="fa fa-twitter"></i>
                <span class="hidden">Twitter</span>
            </a>
        

        
            <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=%2feightpoints%2f"
                onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                <i class="fa fa-facebook"></i>
                <span class="hidden">Facebook</span>
            </a>
        

        
            <a class="icon-google-plus" href="https://plus.google.com/share?url=%2feightpoints%2f"
              onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
              <i class="fa fa-google-plus"></i>
                <span class="hidden">Google+</span>
            </a>
        
        
            <a class="icon-linkedin" href="https://www.linkedin.com/shareArticle?mini=true&title=The%20eight-points%20algorithm&url=%2feightpoints%2f&summary=How%20to%20compute%20the%20transformation%20between%20two%20camera%20poses"
               onclick="window.open(this.href, 'linkedin-share', 'width=554,height=481');return false;">
               <i class="fa fa-linkedin"></i>
               <span class="hidden">LinkedIn</span>
            </a>
        
    </div>
</footer>

        
    </article>
</div>

</div>
</div>

<footer class="footer">
    <div class="container">
        <div class="site-title-wrapper">
            <h1 class="site-title">
                <a title="Lorenzo Peppoloni" href="/">Lorenzo Peppoloni</a>
            </h1>
            <a class="button-square button-jump-top js-jump-top" href="#">
                <i class="fa fa-angle-up"></i>
            </a>
        </div>

        <p class="footer-copyright">
            <span>&copy; 2020 / Powered by <a href="https://gohugo.io/">Hugo</a></span>
        </p>
        <p class="footer-copyright">
            <span><a href="https://github.com/roryg/ghostwriter">Ghostwriter theme</a> By <a href="http://jollygoodthemes.com">JollyGoodThemes</a></span>
            <span>/ <a href="https://github.com/jbub/ghostwriter">Ported</a> to Hugo By <a href="https://github.com/jbub">jbub</a></span>
        </p>
    </div>
</footer>

<script src="/js/jquery-1.11.3.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>


<script src="/js/scripts.js"></script>
</body>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</html>

