<!DOCTYPE html>
<html lang="en-us">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
	<meta name="theme-color" content="#494f5c">
	<meta name="msapplication-TileColor" content="#494f5c">
<meta itemprop="name" content="Effective strategies for classification in CT scans">
<meta itemprop="description" content="Some proven strategies and thoughts to deal with classification of CT scans.">
<meta itemprop="datePublished" content="2020-03-03T07:13:50&#43;00:00" />
<meta itemprop="dateModified" content="2020-03-03T07:13:50&#43;00:00" />
<meta itemprop="wordCount" content="922">



<meta itemprop="keywords" content="computer-vision,machine-learning,deep-learning," /><meta property="og:title" content="Effective strategies for classification in CT scans" />
<meta property="og:description" content="Some proven strategies and thoughts to deal with classification of CT scans." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/posts/dicomscans/" />
<meta property="article:published_time" content="2020-03-03T07:13:50+00:00" />
<meta property="article:modified_time" content="2020-03-03T07:13:50+00:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Effective strategies for classification in CT scans"/>
<meta name="twitter:description" content="Some proven strategies and thoughts to deal with classification of CT scans."/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>Effective strategies for classification in CT scans</title>
	<link rel="stylesheet" href="/css/style.min.eac77496566fd7d5768fd650ddb0b2b181ca6a2d7c5fdd6fe6b8ba4bf47e566f.css" integrity="sha256-6sd0llZv19V2j9ZQ3bCysYHKai18X91v5ri6S/R+Vm8=" crossorigin="anonymous">
	
</head>

<body id="page">
	
	<header id="site-header" class="animated slideInUp">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="/">Lorenzo Peppoloni</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					
				<a href="/posts/">Posts</a>
				<a href="/page/about">About</a>

				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<span class="hdr-social hide-in-mobile"><a href="https://twitter.com/lorepep" target="_blank" rel="noopener me" title="Twitter"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg></a><a href="https://github.com/lorepep" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a><a href="https://www.linkedin.com/in/lorenzo-peppoloni/" target="_blank" rel="noopener me" title="Linkedin"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect x="2" y="9" width="4" height="12"></rect><circle cx="4" cy="4" r="2"></circle></svg></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="/posts/">Posts</a></li>
			<li><a href="/page/about">About</a></li>
		</ul>
	</div>


	<main class="site-main section-inner animated fadeIn faster">
		<article class="thin">
			<header class="post-header">
				<div class="post-meta"><span>Mar 3, 2020</span></div>
				<h1>Effective strategies for classification in CT scans</h1>
			</header>
			<div class="content">
				<p>Last October I took part in the <a href="https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection">RSNA Intracranial Haemorrhage Detection</a> Kaggle challenge. I ended up in the top 10%, which considering my full-time job and travelling, was a placement I am quite happy with.</p>

<p>The goal of this post is to share some ideas and strategies to work with classification in CT scans.</p>

<h2 id="the-task">The task<a href="#the-task" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>

<p>The task in this competition was to tackle a multiclass classification problem, to classify 5 different types of brain haemorrhage (a sixth class was &quot;not present&quot;) from computerized tomography (CT) scans of patient's heads. Each type of haemorrhage tends to appear in different location of the head with different features. A summary with examples is reported below.</p>

<p><figure><img src="/dicom/hemorrage.png" alt="hemorrages"></figure></p>

<p>CT scans usually are slices on the axial plane taken at different heights. This means that you can combine consecutive scans to obtain 3D information. In general, scans are provided in the <strong>DICOM</strong> format, which is an international standard for digital medical images. DICOM files represent pixel intensities in normal units (they can range for example between -32768 and 32767 or less according to the number of bits used).</p>

<h2 id="scans">Scans<a href="#scans" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>

<p>First, you can convert the scans from pixel intensities to Hounsfield Units (HU). Hounsfield Units describe a linear scale of radio intensity. Basically, values range between -1000 (radio intensity of air) and 1000 (roughly radio intensity of metal). Harder materials (such as bone or metal) will have a higher radio intensity. Lighter materials, like flesh, soft tissue or water, will have a lower radio intensity.</p>

<p>To convert from the DICOM to HU you usually have to look for &quot;slope&quot; and &quot;intercept&quot; in the file metadata. The two values, which are usually provided by the manufacturer allow you to get the HU:</p>

<p><span  class="math">\[ \text{scan}_{HU} = \text{scan} * \text{slope} + \text{intercept} \]</span></p>

<p>Now if you try and visualize the images in HU you will probably see something like this</p>

<p><figure><img src="/dicom/hunsfeld.png" alt="hunsfeld_example"></figure></p>

<p>So what's the problem?</p>

<p>The problem is that in a normal grayscale image you can represent 256 different shades, this means that being the HU roughly 2000 values, you have 8 values per shade of grey. As a human, you cannot visually detect changes in shades that are less than 120 HU (in greyscale). That's why you don't see the nice head scans that we were expecting, but instead, you just see a grey blob.</p>

<p>So what do doctors do?</p>

<p>During scan assessment by a human doctor, what is actually done is that each scan is &quot;focused&quot; on a particular range of the Hounsfield Scale, giving information about a certain type of tissue. Doctors usually focus on 2-3 different windows at the same time (according to the assessment they are performing).</p>

<p>In the case of brain haemorrhages, there are 5 important windows, each one focusing on a type of tissue:</p>

<ol>
<li><em>Brain Matter window</em>: W:80 L:40</li>
<li><em>Blood/subdural window</em>: W:130-300 L:50-100</li>
<li><em>Soft tissue window</em>: W:350–400 L:20–60</li>
<li><em>Bone window</em>: W:2800 L:600</li>
<li><em>Grey-white differentiation window</em>: W:8 L:32 or W:40 L:40</li>
</ol>

<p>The windows are expressed with two numbers <code>W</code> the width of the window and <code>L</code> the center. Each window focuses on the range:</p>

<p><span  class="math">\[ L - W / 2 < \text{HU} < L + W / 2 \]</span></p>

<h2 id="mimic-doctors-with-ml">Mimic doctors with ML<a href="#mimic-doctors-with-ml" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>

<p>A viable approach is to choose 3 different windows and use them as the channels of a 3-channel image. In this way our network will try and learn, as a human doctor does, to classify haemorrhage using multiple windows of the same scan.</p>

<p>This approach works and was successfully used during the competition by lots of participants (including me). The approach is also backed up by several research papers.</p>

<p>Some examples of the images one obtains are shown in the pictures.</p>

<p><figure><img src="/dicom/windows.png" alt="windowing"></figure></p>

<p>The images have been min-max scaled to then be fed to the network.</p>

<p><strong>Pros</strong></p>

<ul>
<li>Quite a simple approach.</li>
<li>We are feeding the network the same information a human expert would use (we know it's meaningful).</li>
</ul>

<p><strong>Cons</strong></p>

<ul>
<li>We are dropping information that the network might be able to use.</li>
</ul>

<h2 id="introduce-a-volume-component">Introduce a volume component<a href="#introduce-a-volume-component" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>

<p>Another approach, that was quite successful in the competition was to introduce a volume component instead or together with using multiple windows.</p>

<p>As shown in the figure, scans are consecutive snapshots on the axial plane of the head.</p>

<p><figure><img src="/dicom/scans.png" alt="scans"></figure></p>

<p>Three consecutive scans can be used as the three channels of an RGB image (using still some windowing on the Hounsfield Scale).</p>

<p>An example of the input images (min-max scaled) is shown in the figure.</p>

<p><figure><img src="/dicom/3dvolume.png" alt="volume"></figure></p>

<p><strong>Pros</strong></p>

<ul>
<li>Quite a simple approach.</li>
<li>We are now giving information to the network about volume, although limited.</li>
</ul>

<p><strong>Cons</strong></p>

<ul>
<li>We are dropping information that the network might be able to use (for the windowing).</li>
</ul>

<h2 id="no-windowing">No windowing<a href="#no-windowing" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>

<p>An interesting approach developed during the competition was to drop windows altogether and give the network the full range of HU values. The nuance to tackle with this approach is that the distribution of the pixels over the full range is usually strongly bimodal, with values that are not evenly distributed in the whole range. The distribution can change a lot with the type of tissue that is mainly present in each scan.</p>

<p>A solution to this problem is to find (or craft) a nonlinear normalization function to &quot;normalize&quot; our data over the full range, or almost the full range.</p>

<p>An example can be found in the <a href="https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/discussion/118780">write-up</a> of the 8th place solution to the competition.</p>

<hr>

<p><em>Conclusions: We had a look at some possible approaches that work when dealing with classification in CT scans. We started explaining how scans work, how we can convert them to Hounsfield Units and which strategies we can use to feed the data to a neural network.</em></p>

			</div>
			<hr class="post-end">
			<footer class="post-info">
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-feather"><path d="M20.24 12.24a6 6 0 0 0-8.49-8.49L5 10.5V19h8.5z"></path><line x1="16" y1="8" x2="2" y2="22"></line><line x1="17.5" y1="15" x2="9" y2="15"></line></svg>Lorenzo Peppoloni</p>
				<p>
					<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="/tags/computer-vision">computer-vision</a></span><span class="tag"><a href="/tags/machine-learning">machine-learning</a></span><span class="tag"><a href="/tags/deep-learning">deep-learning</a></span>
				</p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>922 Words</p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2020-03-03 08:13 &#43;0100</p>
			</footer>
		</article>
		<div class="post-nav thin">
			<a class="next-post" href="/posts/gomath/">
				<span class="post-nav-label"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-left"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>&nbsp;Newer</span><br><span>Is GO good at Math?</span>
			</a>
			<a class="prev-post" href="/posts/interfaces/">
				<span class="post-nav-label">Older&nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-right"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg></span><br><span>Do not write misusable APIs</span>
			</a>
		</div>
		<div id="comments" class="thin">
</div>
	</main>

</div>
</div>

<footer class="footer">
    
</footer>

<script src="/js/jquery-1.11.3.min.js"></script>
<script src="/js/jquery.fitvids.js"></script>


    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>


<script src="/js/scripts.js"></script>
</body>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</html>



	<script src="/js/bundle.min.4a9a0ac3d2217822c7865b4161e6c2a71de1d70492264337755427898dd718f6.js" integrity="sha256-SpoKw9IheCLHhltBYebCpx3h1wSSJkM3dVQniY3XGPY=" crossorigin="anonymous"></script>
	
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-159316457-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


</body>

</html>
